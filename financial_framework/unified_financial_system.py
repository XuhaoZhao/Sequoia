from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import time
from datetime import datetime, timedelta
from .industry_sector import IndustrySector
from .stock import Stock
from .etf import ETF
from .concept_sector import ConceptSector
from .index import Index
from .logger_config import LoggerMixin, log_method_call, FinancialLogger
from .file_path_generator import FilePathGenerator
from db_manager import IndustryDataDB
import settings
import push
import pandas as pd
import os
import akshare as ak
import talib
import numpy as np
import json


class UnifiedDataCollector(LoggerMixin):
    """ç»Ÿä¸€æ•°æ®æ”¶é›†å™¨"""

    def __init__(self, db=None):
        """
        Args:
            db: IndustryDataDB æ•°æ®åº“å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å®ä¾‹
        """
        super().__init__()
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        FinancialLogger.setup_logging()

        # åˆå§‹åŒ–æ•°æ®åº“å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        self.db = db if db is not None else IndustryDataDB("industry_data.db")
        self.log_info(f"æ•°æ®åº“å®ä¾‹: {self.db}")

        # åˆå§‹åŒ–å„ç§é‡‘èäº§å“å®ä¾‹ï¼Œæ³¨å…¥åŒä¸€ä¸ªæ•°æ®åº“å®ä¾‹
        self.industry_sector = IndustrySector(self.db)
        self.stock = Stock(self.db)
        self.etf = ETF(self.db)
        self.concept_sector = ConceptSector(self.db)
        self.index = Index(self.db)

        # åˆå§‹åŒ–APScheduler
        self.scheduler = BackgroundScheduler()

        self.log_info("ç»Ÿä¸€æ•°æ®æ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    @log_method_call(include_args=False)
    def collect_all_historical_min_data(self, instrument_type='industry_sector', period="5", delay_seconds=None):
        """æ”¶é›†æŒ‡å®šç±»å‹äº§å“çš„å†å²åˆ†æ—¶æ•°æ®ï¼ˆéå†è¯¥ç±»å‹ä¸‹æ‰€æœ‰å­é¡¹ï¼‰

        Args:
            instrument_type: äº§å“ç±»å‹ ('industry_sector', 'stock', 'etf', 'concept_sector', 'index')
            period: æ•°æ®å‘¨æœŸï¼ˆ"1", "5", "30"ç­‰ï¼Œå•ä½ï¼šåˆ†é’Ÿï¼‰
            delay_seconds: å»¶è¿Ÿç§’æ•°ï¼ˆæ‰¹é‡æ”¶é›†æ—¶ä½¿ç”¨ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å„ç±»çš„é»˜è®¤å»¶è¿Ÿå‚æ•°
        """
        instruments_map = {
            'industry_sector': self.industry_sector,
            'stock': self.stock,
            'etf': self.etf,
            'concept_sector': self.concept_sector,
            'index': self.index
        }

        if instrument_type not in instruments_map:
            self.log_warning(f"æœªçŸ¥çš„äº§å“ç±»å‹: {instrument_type}")
            return

        instrument = instruments_map[instrument_type]

        # å¦‚æœæ²¡æœ‰æŒ‡å®šå»¶è¿Ÿæ—¶é—´ï¼Œä½¿ç”¨ç±»çš„é»˜è®¤å»¶è¿Ÿå‚æ•°
        if delay_seconds is None:
            delay_seconds = instrument.__class__.delay_seconds
            self.log_info(f"ä½¿ç”¨{instrument.get_instrument_type()}çš„é»˜è®¤å»¶è¿Ÿæ—¶é—´: {delay_seconds}ç§’")

        # è°ƒç”¨åŸºç±»çš„ collect_all_historical_min_data æ–¹æ³•
        instrument.collect_all_historical_min_data(period, delay_seconds)

    # ä¿æŒå‘åå…¼å®¹çš„æ–¹æ³•
    def collect_all_historical_5min_data(self, instrument_type='industry_sector', delay_seconds=None):
        """æ”¶é›†æŒ‡å®šç±»å‹äº§å“çš„5åˆ†é’Ÿå†å²æ•°æ®ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰

        Args:
            instrument_type: äº§å“ç±»å‹ ('industry_sector', 'stock', 'etf', 'concept_sector', 'index')
            delay_seconds: å»¶è¿Ÿç§’æ•°ï¼ˆæ‰¹é‡æ”¶é›†æ—¶ä½¿ç”¨ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å„ç±»çš„é»˜è®¤å»¶è¿Ÿå‚æ•°
        """
        return self.collect_all_historical_min_data(instrument_type, "5", delay_seconds)

    @log_method_call(include_args=False)
    def collect_all_daily_data(self, instrument_type='stock', delay_seconds=None):
        """æ”¶é›†æŒ‡å®šç±»å‹äº§å“çš„æ—¥Kæ•°æ®ï¼ˆéå†è¯¥ç±»å‹ä¸‹æ‰€æœ‰å­é¡¹ï¼‰

        Args:
            instrument_type: äº§å“ç±»å‹ ('industry_sector', 'stock', 'etf', 'concept_sector', 'index')
            delay_seconds: å»¶è¿Ÿç§’æ•°ï¼ˆæ‰¹é‡æ”¶é›†æ—¶ä½¿ç”¨ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å„ç±»çš„é»˜è®¤å»¶è¿Ÿå‚æ•°
        """
        instruments_map = {
            'industry_sector': self.industry_sector,
            'stock': self.stock,
            'etf': self.etf,
            'concept_sector': self.concept_sector,
            'index': self.index
        }

        if instrument_type not in instruments_map:
            self.log_warning(f"æœªçŸ¥çš„äº§å“ç±»å‹: {instrument_type}")
            return

        instrument = instruments_map[instrument_type]

        # å¦‚æœæ²¡æœ‰æŒ‡å®šå»¶è¿Ÿæ—¶é—´ï¼Œä½¿ç”¨ç±»çš„é»˜è®¤å»¶è¿Ÿå‚æ•°
        if delay_seconds is None:
            delay_seconds = instrument.__class__.delay_seconds
            self.log_info(f"ä½¿ç”¨{instrument.get_instrument_type()}çš„é»˜è®¤å»¶è¿Ÿæ—¶é—´: {delay_seconds}ç§’")

        # è°ƒç”¨åŸºç±»çš„ collect_all_daily_data æ–¹æ³•
        instrument.collect_all_daily_data(delay_seconds)
    
    @log_method_call(include_args=False)
    def collect_realtime_1min_data(self, instrument_type):
        """æ”¶é›†æŒ‡å®šç±»å‹çš„1åˆ†é’Ÿå®æ—¶æ•°æ®

        Args:
            instrument_type: äº§å“ç±»å‹ ('industry_sector', 'stock', 'etf', 'concept_sector', 'index')
        """
        instruments_map = {
            'industry_sector': self.industry_sector,
            'stock': self.stock,
            'etf': self.etf,
            'concept_sector': self.concept_sector,
            'index': self.index
        }

        if instrument_type not in instruments_map:
            error_msg = f"æœªçŸ¥çš„äº§å“ç±»å‹: {instrument_type}ï¼Œå¿…é¡»æ˜¯ä»¥ä¸‹ç±»å‹ä¹‹ä¸€: {list(instruments_map.keys())}"
            self.log_error(error_msg)
            raise ValueError(error_msg)

        self.log_info(f"å¼€å§‹æ”¶é›†{instrument_type}çš„1åˆ†é’Ÿå®æ—¶æ•°æ® - {datetime.now()}")
        try:
            instruments_map[instrument_type].collect_realtime_1min_data()
            self.log_info(f"{instrument_type}çš„1åˆ†é’Ÿå®æ—¶æ•°æ®æ”¶é›†å®Œæˆ - {datetime.now()}")
        except Exception as e:
            self.log_error(f"{instrument_type}çš„1åˆ†é’Ÿå®æ—¶æ•°æ®æ”¶é›†å¤±è´¥: {e}", exc_info=True)
            raise
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§ç³»ç»Ÿ"""
        # é…ç½®å®šæ—¶ä»»åŠ¡
        # æ¯å¤©æ—©ä¸Š8:00æ”¶é›†å†å²æ•°æ®ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹ä¸ºæ”¶é›†ç‰¹å®šç±»å‹çš„æ•°æ®ï¼‰
        self.scheduler.add_job(
            func=self.collect_all_historical_5min_data,
            trigger=CronTrigger(hour=8, minute=0),
            id='collect_historical_data',
            name='æ”¶é›†5åˆ†é’Ÿå†å²æ•°æ®',
            replace_existing=True,
            kwargs={'instrument_type': 'industry_sector'}
        )

        # æ¯2åˆ†é’Ÿæ”¶é›†å®æ—¶æ•°æ®
        self.scheduler.add_job(
            func=self.collect_realtime_1min_data,
            trigger=CronTrigger(minute='*/2'),
            id='collect_realtime_data',
            name='æ”¶é›†1åˆ†é’Ÿå®æ—¶æ•°æ®',
            replace_existing=True,
            kwargs={'instrument_type': 'index'}
        )

        # å¯åŠ¨è°ƒåº¦å™¨
        self.scheduler.start()

        self.log_info("ç»Ÿä¸€æ•°æ®æ”¶é›†ç³»ç»Ÿå·²å¯åŠ¨...")
        self.log_info("- æ¯å¤©8:00è·å–æ‰€æœ‰äº§å“5åˆ†é’Ÿå†å²æ•°æ®")
        self.log_info("- äº¤æ˜“æ—¶é—´å†…æ¯åˆ†é’Ÿè·å–æ‰€æœ‰äº§å“1åˆ†é’Ÿå®æ—¶æ•°æ®")
        self.log_info(f"- è°ƒåº¦å™¨çŠ¶æ€: {'è¿è¡Œä¸­' if self.scheduler.running else 'å·²åœæ­¢'}")

        try:
            # ä¿æŒç¨‹åºè¿è¡Œ
            while True:
                time.sleep(1)
        except (KeyboardInterrupt, SystemExit):
            self.log_info("æ­£åœ¨åœæ­¢è°ƒåº¦å™¨...")
            self.scheduler.shutdown()
            self.log_info("æ•°æ®å·²ä¿å­˜ï¼Œç¨‹åºé€€å‡º")

    def add_scheduled_job(self, instrument_type, hour=8, minute=0):
        """æ·»åŠ è‡ªå®šä¹‰å®šæ—¶ä»»åŠ¡

        Args:
            instrument_type: äº§å“ç±»å‹ ('industry_sector', 'stock', 'etf', 'concept_sector', 'index')
            hour: æ‰§è¡Œå°æ—¶
            minute: æ‰§è¡Œåˆ†é’Ÿ
        """
        job_id = f'collect_{instrument_type}_data'
        self.scheduler.add_job(
            func=self.collect_all_historical_5min_data,
            trigger=CronTrigger(hour=hour, minute=minute),
            id=job_id,
            name=f'æ”¶é›†{instrument_type}æ•°æ®',
            replace_existing=True,
            kwargs={'instrument_type': instrument_type}
        )
        self.log_info(f"å·²æ·»åŠ å®šæ—¶ä»»åŠ¡: {job_id}ï¼Œæ‰§è¡Œæ—¶é—´ {hour}:{minute:02d}")

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§ç³»ç»Ÿ"""
        if self.scheduler.running:
            self.scheduler.shutdown()
            self.log_info("è°ƒåº¦å™¨å·²åœæ­¢")


class UnifiedAnalyzer:
    """ç»Ÿä¸€åˆ†æå™¨"""

    def __init__(self, db=None):
        """
        Args:
            db: IndustryDataDB æ•°æ®åº“å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å®ä¾‹
        """
        settings.init()

        # åˆå§‹åŒ–æ•°æ®åº“å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        self.db = db if db is not None else IndustryDataDB("industry_data.db")

        # åˆå§‹åŒ–å„ç§é‡‘èäº§å“å®ä¾‹ï¼Œæ³¨å…¥åŒä¸€ä¸ªæ•°æ®åº“å®ä¾‹
        self.industry_sector = IndustrySector(self.db)
        self.stock = Stock(self.db)
        self.etf = ETF(self.db)
        self.concept_sector = ConceptSector(self.db)
        self.index = Index(self.db)
    
    def analyze_instrument(self, instrument_type, instrument_info):
        """åˆ†ææŒ‡å®šäº§å“"""
        instruments_map = {
            'industry_sector': self.industry_sector,
            'stock': self.stock,
            'etf': self.etf,
            'concept_sector': self.concept_sector,
            'index': self.index
        }
        
        if instrument_type in instruments_map:
            try:
                instruments_map[instrument_type].analyze_macd(instrument_info)
            except Exception as e:
                print(f"åˆ†æ{instrument_info.get('name', '')}å¤±è´¥: {e}")
    
    def analyze_all_instruments(self, instrument_type='industry_sector'):
        """åˆ†ææŒ‡å®šç±»å‹çš„æ‰€æœ‰äº§å“ï¼Œæ”¶é›†æ‰€æœ‰é‡‘å‰ä¿¡å·åç»Ÿä¸€ä¿å­˜

        Args:
            instrument_type: äº§å“ç±»å‹ ('industry_sector', 'stock', 'etf', 'concept_sector', 'index')
        """
        instruments_map = {
            'industry_sector': self.industry_sector,
            'stock': self.stock,
            'etf': self.etf,
            'concept_sector': self.concept_sector,
            'index': self.index
        }

        if instrument_type not in instruments_map:
            print(f"æœªçŸ¥çš„äº§å“ç±»å‹: {instrument_type}")
            return

        instrument = instruments_map[instrument_type]
        print(f"å¼€å§‹åˆ†æ{instrument.get_instrument_type()}...")

        # æ”¶é›†æ‰€æœ‰é‡‘å‰ä¿¡å·æ•°æ®
        all_golden_cross_data = []

        all_instruments = instrument.get_all_instruments()
        for instrument_info in all_instruments:
            try:
                # analyze_macd ç°åœ¨è¿”å›é‡‘å‰ä¿¡å·æ•°æ®åˆ—è¡¨
                golden_cross_data = instrument.analyze_macd(instrument_info, instrument_type)
                if golden_cross_data:
                    all_golden_cross_data.extend(golden_cross_data)
            except Exception as e:
                print(f"åˆ†æ{instrument_info.get('name', '')}å¤±è´¥: {e}")

        # ç»Ÿä¸€ä¿å­˜æ‰€æœ‰é‡‘å‰ä¿¡å·åˆ°CSV
        if all_golden_cross_data:
            today = datetime.now().strftime('%Y-%m-%d')
            self._save_golden_cross_to_csv(all_golden_cross_data, instrument_type, today)
            print(f"å…±æ”¶é›†åˆ° {len(all_golden_cross_data)} ä¸ªé‡‘å‰ä¿¡å·ï¼Œå·²ä¿å­˜åˆ°CSV")
        else:
            print("æœªå‘ç°é‡‘å‰ä¿¡å·")

        print(f"{instrument.get_instrument_type()}åˆ†æå®Œæˆ")

    def _save_golden_cross_to_csv(self, data, instrument_type, date_str):
        """å°†é‡‘å‰ä¿¡å·ä¿å­˜åˆ°CSVæ–‡ä»¶

        Args:
            data: é‡‘å‰ä¿¡å·æ•°æ®åˆ—è¡¨
            instrument_type: äº§å“ç±»å‹
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
        """
        try:
            # ä½¿ç”¨FilePathGeneratorç”Ÿæˆæ–‡ä»¶è·¯å¾„
            filepath = FilePathGenerator.generate_macd_signal_path(
                instrument_type=instrument_type,
                period="30m",
                date=date_str
            )

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            FilePathGenerator.ensure_directory_exists(filepath)

            # åˆ›å»ºDataFrame
            df = pd.DataFrame(data)

            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ æ•°æ®ï¼›å¦åˆ™åˆ›å»ºæ–°æ–‡ä»¶
            if os.path.exists(filepath):
                # è¯»å–ç°æœ‰æ•°æ®
                existing_df = pd.read_csv(filepath)
                # åˆå¹¶æ•°æ®å¹¶å»é‡(åŸºäºcodeå’Œtime)
                df = pd.concat([existing_df, df], ignore_index=True)
                df = df.drop_duplicates(subset=['code', 'time'], keep='last')

            # ä¿å­˜åˆ°CSV
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"é‡‘å‰ä¿¡å·å·²ä¿å­˜åˆ°æ–‡ä»¶: {filepath}")

        except Exception as e:
            print(f"ä¿å­˜é‡‘å‰ä¿¡å·åˆ°CSVå¤±è´¥: {e}")

    def run_analysis(self, instrument_type='industry_sector'):
        """è¿è¡Œåˆ†æ

        Args:
            instrument_type: äº§å“ç±»å‹ ('industry_sector', 'stock', 'etf', 'concept_sector', 'index')
        """
        print("å¼€å§‹ç»Ÿä¸€åˆ†æ...")
        self.analyze_all_instruments(instrument_type)
        print("ç»Ÿä¸€åˆ†æå®Œæˆ")


class TechnicalAnalyzer:
    """
    æŠ€æœ¯åˆ†æç±»ï¼ˆä» industry_daliyK_analysis.py æ•´åˆï¼‰
    æä¾›ç»¼åˆæŠ€æœ¯åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - ZigZagåˆ†æ
    - åˆ†å½¢åˆ†æ
    - æ–æ³¢é‚£å¥‘å›æ’¤
    - å¸ƒæ—å¸¦åˆ†æ
    - ç§»åŠ¨å¹³å‡çº¿åˆ†æ
    - è½¬æŠ˜ç‚¹æ£€æµ‹
    """

    def __init__(self, db=None, default_symbol="äººå½¢æœºå™¨äºº", default_days_back=250, default_data_source="industry"):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            db: IndustryDataDB æ•°æ®åº“å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å®ä¾‹
            default_symbol: é»˜è®¤åˆ†æçš„æ¿å—åç§°æˆ–è‚¡ç¥¨ä»£ç 
            default_days_back: é»˜è®¤åˆ†æå¤©æ•°
            default_data_source: é»˜è®¤æ•°æ®æ¥æº ("industry", "stock", "concept")
        """
        # åˆå§‹åŒ–æ•°æ®åº“å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        self.db = db if db is not None else IndustryDataDB("industry_data.db")
        self.default_symbol = default_symbol
        self.default_days_back = default_days_back
        self.default_data_source = default_data_source

    def zigzag(self, high, low, close, deviation=0.05):
        """
        ZigZagç®—æ³•è¯†åˆ«é«˜ä½ç‚¹

        Args:
            high, low, close: ä»·æ ¼æ•°ç»„
            deviation: æœ€å°å˜åŒ–å¹…åº¦ï¼ˆé»˜è®¤5%ï¼‰

        Returns:
            list: [(index, price, type)] typeä¸º'high'æˆ–'low'
        """
        peaks = []
        if len(close) < 3:
            return peaks

        trend = None
        last_peak_idx = 0
        last_peak_price = close[0]

        for i in range(1, len(close)):
            if trend is None:
                if close[i] > close[i-1] * (1 + deviation):
                    trend = 'up'
                    last_peak_idx = i-1
                    last_peak_price = close[i-1]
                    peaks.append((i-1, close[i-1], 'low'))
                elif close[i] < close[i-1] * (1 - deviation):
                    trend = 'down'
                    last_peak_idx = i-1
                    last_peak_price = close[i-1]
                    peaks.append((i-1, close[i-1], 'high'))

            elif trend == 'up':
                if close[i] > last_peak_price:
                    last_peak_idx = i
                    last_peak_price = close[i]
                elif close[i] < last_peak_price * (1 - deviation):
                    peaks.append((last_peak_idx, last_peak_price, 'high'))
                    trend = 'down'
                    last_peak_idx = i
                    last_peak_price = close[i]

            elif trend == 'down':
                if close[i] < last_peak_price:
                    last_peak_idx = i
                    last_peak_price = close[i]
                elif close[i] > last_peak_price * (1 + deviation):
                    peaks.append((last_peak_idx, last_peak_price, 'low'))
                    trend = 'up'
                    last_peak_idx = i
                    last_peak_price = close[i]

        if trend and len(peaks) > 0:
            peaks.append((last_peak_idx, last_peak_price, 'high' if trend == 'up' else 'low'))

        return peaks


    def fractal_highs_lows(self, high, low, period=2):
        """
        åˆ†å½¢ç®—æ³•è¯†åˆ«å±€éƒ¨é«˜ä½ç‚¹

        Args:
            high, low: ä»·æ ¼æ•°ç»„
            period: åˆ†å½¢å‘¨æœŸï¼ˆé»˜è®¤2ï¼Œå³å‰å2ä¸ªç‚¹ï¼‰

        Returns:
            dict: {'highs': [(index, price)], 'lows': [(index, price)]}
        """
        fractal_highs = []
        fractal_lows = []

        for i in range(period, len(high) - period):
            is_high = True
            is_low = True

            for j in range(i - period, i + period + 1):
                if j == i:
                    continue
                if high[j] >= high[i]:
                    is_high = False
                if low[j] <= low[i]:
                    is_low = False

            if is_high:
                fractal_highs.append((i, high[i]))
            if is_low:
                fractal_lows.append((i, low[i]))

        return {'highs': fractal_highs, 'lows': fractal_lows}


    def fibonacci_retracement(self, high_price, low_price):
        """
        è®¡ç®—æ–æ³¢é‚£å¥‘å›æ’¤ä½

        Args:
            high_price: é«˜ç‚¹ä»·æ ¼
            low_price: ä½ç‚¹ä»·æ ¼

        Returns:
            dict: å„ä¸ªæ–æ³¢é‚£å¥‘å›æ’¤ä½
        """
        price_range = high_price - low_price

        fib_levels = {
            '0%': high_price,
            '23.6%': high_price - price_range * 0.236,
            '38.2%': high_price - price_range * 0.382,
            '50%': high_price - price_range * 0.5,
            '61.8%': high_price - price_range * 0.618,
            '78.6%': high_price - price_range * 0.786,
            '100%': low_price
        }

        return fib_levels


    def analyze_comprehensive_technical(self, code=None, symbol=None, days_back=None, data_source=None):
        """
        ç»¼åˆæŠ€æœ¯åˆ†æï¼šå¸ƒæ—å¸¦ + æ–æ³¢é‚£å¥‘å›æ’¤ + ZigZag + åˆ†å½¢

        Args:
            code: æ¿å—ä»£ç æˆ–è‚¡ç¥¨ä»£ç ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            symbol: æ¿å—åç§°æˆ–è‚¡ç¥¨ä»£ç ï¼ˆå½“codeä¸ºNoneæ—¶ä½¿ç”¨ï¼Œé»˜è®¤ä½¿ç”¨å®ä¾‹åŒ–æ—¶çš„é»˜è®¤å€¼ï¼‰
            days_back: åˆ†æå¤©æ•°ï¼ˆé»˜è®¤ä½¿ç”¨å®ä¾‹åŒ–æ—¶çš„é»˜è®¤å€¼ï¼‰
            data_source: æ•°æ®æ¥æºï¼Œå¯é€‰å€¼ï¼š
                - "industry": è¡Œä¸šæ¿å—æ•°æ®
                - "stock": ä¸ªè‚¡æ•°æ®
                - "concept": æ¦‚å¿µæ¿å—æ•°æ®
                ï¼ˆé»˜è®¤ä½¿ç”¨å®ä¾‹åŒ–æ—¶çš„é»˜è®¤å€¼ï¼‰

        Returns:
            dict: åŒ…å«æ‰€æœ‰æŠ€æœ¯åˆ†æç»“æœçš„å­—å…¸
        """
        # å‚æ•°å¤„ç†
        if code is None and symbol is None:
            symbol = self.default_symbol
        if days_back is None:
            days_back = self.default_days_back
        if data_source is None:
            data_source = self.default_data_source

        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")

        # ä»æ•°æ®åº“è·å–æ—¥Kæ•°æ®
        try:
            # å¦‚æœæä¾›äº†codeï¼Œç›´æ¥ä½¿ç”¨codeæŸ¥è¯¢
            query_code = code if code is not None else symbol

            # ä»æ•°æ®åº“æŸ¥è¯¢æ—¥Kæ•°æ®
            df = self.db.query_kline_data('1d', code=query_code, start_date=start_date, end_date=end_date)

            if df is None or df.empty:
                return {"error": f"æ— æ³•ä»æ•°æ®åº“è·å–æ—¥Kæ•°æ®ï¼Œcode/symbol: {query_code}"}

            # é‡å‘½ååˆ—ä»¥åŒ¹é…åç»­å¤„ç†
            df = df.rename(columns={
                'datetime': 'æ—¥æœŸ',
                'open_price': 'å¼€ç›˜',
                'high_price': 'æœ€é«˜',
                'low_price': 'æœ€ä½',
                'close_price': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡'
            })

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])

        except Exception as e:
            return {"error": f"è·å–æ•°æ®åº“æ—¥Kæ•°æ®å¤±è´¥: {str(e)}"}

        df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)

        high_prices = df['æœ€é«˜'].values.astype(float)
        low_prices = df['æœ€ä½'].values.astype(float)
        close_prices = df['æ”¶ç›˜'].values.astype(float)
        volumes = df['æˆäº¤é‡'].values.astype(float)

        upper_band, middle_band, lower_band = talib.BBANDS(
            close_prices,
            timeperiod=20,
            nbdevup=2,
            nbdevdn=2,
            matype=0
        )

        df['ä¸Šè½¨'] = upper_band
        df['ä¸­è½¨'] = middle_band
        df['ä¸‹è½¨'] = lower_band

        ma_data = self.calculate_moving_averages(close_prices)
        for ma_name, ma_values in ma_data.items():
            df[ma_name] = ma_values

        # è®¡ç®—æˆäº¤é‡åˆ†æ
        volume_analysis = self.analyze_volume_status(volumes, lookback_days=60)

        zigzag_points = self.zigzag(high_prices, low_prices, close_prices, deviation=0.08)

        fractals = self.fractal_highs_lows(high_prices, low_prices, period=3)

        latest_data = df.iloc[-1]
        latest_close = float(latest_data['æ”¶ç›˜'])
        latest_lower_band = float(latest_data['ä¸‹è½¨'])
        latest_middle_band = float(latest_data['ä¸­è½¨'])
        latest_upper_band = float(latest_data['ä¸Šè½¨'])

        ma_arrangement = self.analyze_ma_arrangement(ma_data, latest_close)
        crossover_signals = self.detect_ma_crossover_signals(ma_data, lookback=5)
        turning_points = self.detect_turning_points(close_prices, ma_data, latest_close)

        bb_is_oversold = latest_close < latest_lower_band

        distance_to_lower = ((latest_close - latest_lower_band) / latest_lower_band) * 100
        bb_position = ((latest_close - latest_lower_band) / (latest_upper_band - latest_lower_band)) * 100

        recent_highs = [point for point in zigzag_points if point[2] == 'high'][-3:]
        recent_lows = [point for point in zigzag_points if point[2] == 'low'][-3:]

        fib_analysis = {}
        if recent_highs and recent_lows:
            last_high = max(recent_highs, key=lambda x: x[1])
            last_low = min(recent_lows, key=lambda x: x[1])

            if last_high[0] > last_low[0]:
                swing_high = last_high[1]
                swing_low = last_low[1]
                fib_levels = self.fibonacci_retracement(swing_high, swing_low)

                fib_support_levels = []
                for level, price in fib_levels.items():
                    if abs(latest_close - price) / price < 0.02:
                        fib_support_levels.append(level)

                fib_analysis = {
                    "æ‘†åŠ¨é«˜ç‚¹": swing_high,
                    "æ‘†åŠ¨ä½ç‚¹": swing_low,
                    "æ–æ³¢é‚£å¥‘å›æ’¤ä½": fib_levels,
                    "å½“å‰ä½ç½®æ¥è¿‘çš„å›æ’¤ä½": fib_support_levels,
                    "å›æ’¤ç™¾åˆ†æ¯”": ((swing_high - latest_close) / (swing_high - swing_low)) * 100 if swing_high != swing_low else 0
                }

        fractal_recent_highs = fractals['highs'][-5:] if len(fractals['highs']) >= 5 else fractals['highs']
        fractal_recent_lows = fractals['lows'][-5:] if len(fractals['lows']) >= 5 else fractals['lows']

        ç»¼åˆåˆ†æä¿¡å· = []

        if bb_is_oversold:
            ç»¼åˆåˆ†æä¿¡å·.append("å¸ƒæ—å¸¦ä¸‹è½¨è¶…è·Œ")

        if bb_position < 20:
            ç»¼åˆåˆ†æä¿¡å·.append("å¸ƒæ—å¸¦åº•éƒ¨åŒºåŸŸ")

        if fib_analysis.get("å›æ’¤ç™¾åˆ†æ¯”", 0) > 50:
            ç»¼åˆåˆ†æä¿¡å·.append("æ–æ³¢é‚£å¥‘æ·±åº¦å›æ’¤")

        if fib_analysis.get("å½“å‰ä½ç½®æ¥è¿‘çš„å›æ’¤ä½"):
            ç»¼åˆåˆ†æä¿¡å·.append(f"æ¥è¿‘æ–æ³¢é‚£å¥‘æ”¯æ’‘ä½: {', '.join(fib_analysis['å½“å‰ä½ç½®æ¥è¿‘çš„å›æ’¤ä½'])}")

        if len(recent_lows) > 0:
            last_zigzag_low = min(recent_lows, key=lambda x: x[1])[1]
            if latest_close <= last_zigzag_low * 1.05:
                ç»¼åˆåˆ†æä¿¡å·.append("æ¥è¿‘ZigZagå…³é”®ä½ç‚¹")

        if len(fractal_recent_lows) > 0:
            last_fractal_low = min(fractal_recent_lows, key=lambda x: x[1])[1]
            if latest_close <= last_fractal_low * 1.03:
                ç»¼åˆåˆ†æä¿¡å·.append("æ¥è¿‘åˆ†å½¢å…³é”®ä½ç‚¹")

        if ma_arrangement["æ’åˆ—çŠ¶æ€"] in ["å®Œç¾å¤šå¤´æ’åˆ—", "å¤šå¤´æ’åˆ—"]:
            ç»¼åˆåˆ†æä¿¡å·.append(f"å‡çº¿å‘ˆ{ma_arrangement['æ’åˆ—çŠ¶æ€']}")
        elif ma_arrangement["æ’åˆ—çŠ¶æ€"] in ["å®Œç¾ç©ºå¤´æ’åˆ—", "ç©ºå¤´æ’åˆ—"]:
            ç»¼åˆåˆ†æä¿¡å·.append(f"å‡çº¿å‘ˆ{ma_arrangement['æ’åˆ—çŠ¶æ€']}")

        for signal in crossover_signals:
            if signal["å¤©æ•°å‰"] <= 3:
                ç»¼åˆåˆ†æä¿¡å·.append(f"{signal['å¤©æ•°å‰']}å¤©å‰{signal['å¿«çº¿']}{signal['ç±»å‹']}{signal['æ…¢çº¿']}")

        if turning_points["ç»¼åˆåˆ¤æ–­"] == "å…³é”®è½¬æŠ˜ç‚¹":
            ç»¼åˆåˆ†æä¿¡å·.append("æ£€æµ‹åˆ°å…³é”®è½¬æŠ˜ç‚¹ä¿¡å·")

        # æ·»åŠ æˆäº¤é‡ç›¸å…³çš„åˆ†æä¿¡å·
        if "error" not in volume_analysis:
            if volume_analysis["æˆäº¤é‡çŠ¶æ€"] == "æä½":
                ç»¼åˆåˆ†æä¿¡å·.append(f"æˆäº¤é‡å¤„äº{volume_analysis['æˆäº¤é‡ç™¾åˆ†ä½']:.1f}åˆ†ä½ï¼Œæåº¦èç¼©")
            elif volume_analysis["æˆäº¤é‡çŠ¶æ€"] == "ä½":
                ç»¼åˆåˆ†æä¿¡å·.append(f"æˆäº¤é‡å¤„äº{volume_analysis['æˆäº¤é‡ç™¾åˆ†ä½']:.1f}åˆ†ä½ï¼Œæ˜æ˜¾èç¼©")

            if volume_analysis["æˆäº¤é‡è¶‹åŠ¿"] in ["æ˜æ˜¾æ”¾é‡", "æ”¾é‡"] and volume_analysis["æˆäº¤é‡ç­‰çº§"] <= 2:
                ç»¼åˆåˆ†æä¿¡å·.append(f"åº•éƒ¨åŒºåŸŸå‡ºç°{volume_analysis['æˆäº¤é‡è¶‹åŠ¿']}({volume_analysis['æˆäº¤é‡å˜åŒ–ç‡']:+.1f}%)")

        ç»¼åˆè¯„çº§ = "å¼ºçƒˆè¶…è·Œ" if len(ç»¼åˆåˆ†æä¿¡å·) >= 3 else "å¯èƒ½è¶…è·Œ" if len(ç»¼åˆåˆ†æä¿¡å·) >= 2 else "è§‚æœ›" if len(ç»¼åˆåˆ†æä¿¡å·) >= 1 else "æ­£å¸¸"

        return {
            "æ¿å—åç§°": query_code,
            "æœ€æ–°æ—¥æœŸ": latest_data['æ—¥æœŸ'],
            "æœ€æ–°æ”¶ç›˜ä»·": latest_close,

            "å‡çº¿åˆ†æ": ma_arrangement,

            "å‡çº¿äº¤å‰ä¿¡å·": crossover_signals,

            "è½¬æŠ˜ç‚¹åˆ†æ": turning_points,

            "æˆäº¤é‡åˆ†æ": volume_analysis,

            "å¸ƒæ—å¸¦åˆ†æ": {
                "ä¸Šè½¨": latest_upper_band,
                "ä¸­è½¨": latest_middle_band,
                "ä¸‹è½¨": latest_lower_band,
                "æ˜¯å¦è¶…è·Œ": bb_is_oversold,
                "è·ç¦»ä¸‹è½¨ç™¾åˆ†æ¯”": round(distance_to_lower, 2),
                "å¸ƒæ—å¸¦ä½ç½®": round(bb_position, 2)
            },

            "ZigZagåˆ†æ": {
                "æœ€è¿‘é«˜ç‚¹": recent_highs,
                "æœ€è¿‘ä½ç‚¹": recent_lows,
                "å…³é”®ç‚¹æ•°é‡": len(zigzag_points)
            },

            "åˆ†å½¢åˆ†æ": {
                "åˆ†å½¢é«˜ç‚¹": fractal_recent_highs,
                "åˆ†å½¢ä½ç‚¹": fractal_recent_lows
            },

            "æ–æ³¢é‚£å¥‘åˆ†æ": fib_analysis,

            "ç»¼åˆåˆ†æä¿¡å·": ç»¼åˆåˆ†æä¿¡å·,
            "ç»¼åˆè¯„çº§": ç»¼åˆè¯„çº§,
            "æŠ•èµ„å»ºè®®": self.get_investment_advice(ç»¼åˆè¯„çº§, len(ç»¼åˆåˆ†æä¿¡å·))
        }


    def calculate_moving_averages(self, prices, periods=[5, 10, 20, 30, 60]):
        """
        è®¡ç®—å¤šå‘¨æœŸç§»åŠ¨å¹³å‡çº¿

        Args:
            prices: ä»·æ ¼åºåˆ—
            periods: å‡çº¿å‘¨æœŸåˆ—è¡¨

        Returns:
            dict: å„å‘¨æœŸå‡çº¿æ•°æ®
        """
        ma_data = {}
        for period in periods:
            ma_data[f'MA{period}'] = talib.SMA(prices, timeperiod=period)
        return ma_data


    def analyze_ma_arrangement(self, ma_data, current_price):
        """
        åˆ†æå‡çº¿æ’åˆ—çŠ¶æ€

        Args:
            ma_data: å‡çº¿æ•°æ®å­—å…¸
            current_price: å½“å‰ä»·æ ¼

        Returns:
            dict: æ’åˆ—åˆ†æç»“æœ
        """
        periods = [5, 10, 20, 30, 60]
        ma_values = []

        for period in periods:
            ma_key = f'MA{period}'
            if ma_key in ma_data and not np.isnan(ma_data[ma_key][-1]):
                ma_values.append((period, ma_data[ma_key][-1]))

        if len(ma_values) < 3:
            return {"æ’åˆ—çŠ¶æ€": "æ•°æ®ä¸è¶³", "ä¿¡å·å¼ºåº¦": 0}

        ma_values_only = [value for _, value in ma_values]

        is_bullish = all(ma_values_only[i] >= ma_values_only[i+1] for i in range(len(ma_values_only)-1))
        is_bearish = all(ma_values_only[i] <= ma_values_only[i+1] for i in range(len(ma_values_only)-1))

        price_above_all = current_price > max(ma_values_only)
        price_below_all = current_price < min(ma_values_only)

        if is_bullish and price_above_all:
            arrangement = "å®Œç¾å¤šå¤´æ’åˆ—"
            signal_strength = 5
        elif is_bullish:
            arrangement = "å¤šå¤´æ’åˆ—"
            signal_strength = 4
        elif is_bearish and price_below_all:
            arrangement = "å®Œç¾ç©ºå¤´æ’åˆ—"
            signal_strength = -5
        elif is_bearish:
            arrangement = "ç©ºå¤´æ’åˆ—"
            signal_strength = -4
        else:
            arrangement = "æ··ä¹±æ’åˆ—"
            signal_strength = 0

        return {
            "æ’åˆ—çŠ¶æ€": arrangement,
            "ä¿¡å·å¼ºåº¦": signal_strength,
            "ä»·æ ¼ä½ç½®": "å¤šå¤´" if current_price > ma_values_only[0] else "ç©ºå¤´",
            "å‡çº¿æ•°å€¼": {f'MA{period}': round(value, 2) for period, value in ma_values}
        }


    def detect_ma_crossover_signals(self, ma_data, lookback=5):
        """
        æ£€æµ‹å‡çº¿äº¤å‰ä¿¡å·

        Args:
            ma_data: å‡çº¿æ•°æ®å­—å…¸
            lookback: å›çœ‹å¤©æ•°

        Returns:
            list: äº¤å‰ä¿¡å·åˆ—è¡¨
        """
        signals = []
        periods = [5, 10, 20, 30, 60]

        for i in range(len(periods)):
            for j in range(i+1, len(periods)):
                fast_period = periods[i]
                slow_period = periods[j]

                fast_ma = ma_data[f'MA{fast_period}']
                slow_ma = ma_data[f'MA{slow_period}']

                if len(fast_ma) < lookback or len(slow_ma) < lookback:
                    continue

                for k in range(1, min(lookback, len(fast_ma))):
                    if (fast_ma[-k-1] <= slow_ma[-k-1] and fast_ma[-k] > slow_ma[-k]):
                        signals.append({
                            "ç±»å‹": "é‡‘å‰",
                            "å¿«çº¿": f"MA{fast_period}",
                            "æ…¢çº¿": f"MA{slow_period}",
                            "å‘ç”Ÿä½ç½®": len(fast_ma) - k,
                            "å¤©æ•°å‰": k,
                            "ä¿¡å·å¼ºåº¦": "å¼º" if fast_period <= 10 and slow_period >= 20 else "ä¸­"
                        })
                    elif (fast_ma[-k-1] >= slow_ma[-k-1] and fast_ma[-k] < slow_ma[-k]):
                        signals.append({
                            "ç±»å‹": "æ­»å‰",
                            "å¿«çº¿": f"MA{fast_period}",
                            "æ…¢çº¿": f"MA{slow_period}",
                            "å‘ç”Ÿä½ç½®": len(fast_ma) - k,
                            "å¤©æ•°å‰": k,
                            "ä¿¡å·å¼ºåº¦": "å¼º" if fast_period <= 10 and slow_period >= 20 else "ä¸­"
                        })

        return sorted(signals, key=lambda x: x["å¤©æ•°å‰"])


    def detect_turning_points(self, prices, ma_data, current_price):
        """
        æ£€æµ‹æ½œåœ¨è½¬æŠ˜ç‚¹

        Args:
            prices: ä»·æ ¼åºåˆ—
            ma_data: å‡çº¿æ•°æ®
            current_price: å½“å‰ä»·æ ¼

        Returns:
            dict: è½¬æŠ˜ç‚¹åˆ†æç»“æœ
        """
        signals = []

        ma5 = ma_data.get('MA5', [])
        ma10 = ma_data.get('MA10', [])
        ma20 = ma_data.get('MA20', [])

        if len(ma5) < 3 or len(ma10) < 3 or len(ma20) < 3:
            return {"è½¬æŠ˜ä¿¡å·": [], "ç»¼åˆåˆ¤æ–­": "æ•°æ®ä¸è¶³"}

        ma5_slope = (ma5[-1] - ma5[-3]) / 2
        ma10_slope = (ma10[-1] - ma10[-3]) / 2
        ma20_slope = (ma20[-1] - ma20[-3]) / 2

        if ma5_slope > 0 and ma10_slope > 0 and current_price > ma5[-1]:
            if ma20_slope <= 0:
                signals.append("çŸ­æœŸå‡çº¿å‘ä¸Šï¼Œå¯èƒ½å½¢æˆåº•éƒ¨")
            else:
                signals.append("å¤šå‡çº¿å‘ä¸Šï¼Œä¸Šå‡è¶‹åŠ¿ç¡®è®¤")

        if ma5_slope < 0 and ma10_slope < 0 and current_price < ma5[-1]:
            if ma20_slope >= 0:
                signals.append("çŸ­æœŸå‡çº¿å‘ä¸‹ï¼Œå¯èƒ½å½¢æˆé¡¶éƒ¨")
            else:
                signals.append("å¤šå‡çº¿å‘ä¸‹ï¼Œä¸‹é™è¶‹åŠ¿ç¡®è®¤")

        price_volatility = np.std(prices[-10:]) / np.mean(prices[-10:])
        if price_volatility > 0.05:
            signals.append("ä»·æ ¼æ³¢åŠ¨åŠ å‰§ï¼Œæ³¨æ„è¶‹åŠ¿è½¬æ¢")

        ma_convergence = abs(ma5[-1] - ma20[-1]) / ma20[-1]
        if ma_convergence < 0.02:
            signals.append("å‡çº¿æ”¶æ•›ï¼Œå…³æ³¨çªç ´æ–¹å‘")

        if len(signals) >= 2:
            trend_judgment = "å…³é”®è½¬æŠ˜ç‚¹"
        elif len(signals) == 1:
            trend_judgment = "æ½œåœ¨è½¬æŠ˜"
        else:
            trend_judgment = "è¶‹åŠ¿å»¶ç»­"

        return {
            "è½¬æŠ˜ä¿¡å·": signals,
            "ç»¼åˆåˆ¤æ–­": trend_judgment,
            "å‡çº¿æ–œç‡": {
                "MA5æ–œç‡": round(ma5_slope, 4),
                "MA10æ–œç‡": round(ma10_slope, 4),
                "MA20æ–œç‡": round(ma20_slope, 4)
            }
        }


    def calculate_volume_ma(self, volumes, periods=[5, 10, 20]):
        """
        è®¡ç®—æˆäº¤é‡ç§»åŠ¨å¹³å‡çº¿

        Args:
            volumes: æˆäº¤é‡åºåˆ—
            periods: å‡çº¿å‘¨æœŸåˆ—è¡¨ï¼Œé»˜è®¤[5, 10, 20]

        Returns:
            dict: å„å‘¨æœŸæˆäº¤é‡å‡çº¿æ•°æ®
        """
        volume_ma_data = {}
        for period in periods:
            volume_ma_data[f'VMA{period}'] = talib.SMA(volumes, timeperiod=period)
        return volume_ma_data


    def analyze_volume_status(self, volumes, lookback_days=60):
        """
        åˆ†ææˆäº¤é‡çŠ¶æ€ï¼Œåˆ¤æ–­å½“å‰5æ—¥æˆäº¤é‡å‡çº¿æ˜¯å¤„äºä½ç‚¹è¿˜æ˜¯é«˜ç‚¹

        Args:
            volumes: æˆäº¤é‡åºåˆ—
            lookback_days: å›çœ‹å¤©æ•°ï¼Œç”¨äºåˆ¤æ–­é«˜ä½ç‚¹ï¼Œé»˜è®¤60å¤©

        Returns:
            dict: æˆäº¤é‡åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - current_vma5: å½“å‰5æ—¥æˆäº¤é‡å‡çº¿
                - vma5_percentile: 5æ—¥æˆäº¤é‡å‡çº¿åœ¨å›çœ‹æœŸå†…çš„ç™¾åˆ†ä½
                - volume_status: æˆäº¤é‡çŠ¶æ€ï¼ˆæä½ã€ä½ã€ä¸­ç­‰ã€é«˜ã€æé«˜ï¼‰
                - volume_trend: æˆäº¤é‡è¶‹åŠ¿ï¼ˆæ”¾é‡ã€ç¼©é‡ã€å¹³ç¨³ï¼‰
                - max_vma5: å›çœ‹æœŸå†…5æ—¥å‡çº¿æœ€å¤§å€¼
                - min_vma5: å›çœ‹æœŸå†…5æ—¥å‡çº¿æœ€å°å€¼
        """
        if len(volumes) < 5:
            return {"error": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—5æ—¥æˆäº¤é‡å‡çº¿"}

        # è®¡ç®—5æ—¥æˆäº¤é‡å‡çº¿
        vma5 = talib.SMA(volumes, timeperiod=5)

        if len(vma5) < lookback_days:
            lookback_days = len(vma5)

        # è·å–å›çœ‹æœŸå†…çš„æ•°æ®
        recent_vma5 = vma5[-lookback_days:]
        current_vma5 = vma5[-1]

        # å»é™¤NaNå€¼
        valid_vma5 = recent_vma5[~np.isnan(recent_vma5)]

        if len(valid_vma5) == 0:
            return {"error": "æœ‰æ•ˆæ•°æ®ä¸è¶³"}

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        max_vma5 = np.max(valid_vma5)
        min_vma5 = np.min(valid_vma5)
        mean_vma5 = np.mean(valid_vma5)
        std_vma5 = np.std(valid_vma5)

        # è®¡ç®—å½“å‰å€¼çš„ç™¾åˆ†ä½ï¼ˆåœ¨å›çœ‹æœŸå†…çš„ä½ç½®ï¼‰
        percentile = np.sum(valid_vma5 <= current_vma5) / len(valid_vma5) * 100

        # åˆ¤æ–­æˆäº¤é‡çŠ¶æ€
        if percentile <= 20:
            volume_status = "æä½"
            volume_level = 1
        elif percentile <= 40:
            volume_status = "ä½"
            volume_level = 2
        elif percentile <= 60:
            volume_status = "ä¸­ç­‰"
            volume_level = 3
        elif percentile <= 80:
            volume_status = "é«˜"
            volume_level = 4
        else:
            volume_status = "æé«˜"
            volume_level = 5

        # åˆ†ææˆäº¤é‡è¶‹åŠ¿ï¼ˆå¯¹æ¯”å‰ä¸€ä¸ª5æ—¥å‡çº¿ï¼‰
        if len(vma5) >= 2 and not np.isnan(vma5[-2]):
            prev_vma5 = vma5[-2]
            volume_change_pct = ((current_vma5 - prev_vma5) / prev_vma5) * 100

            if volume_change_pct > 10:
                volume_trend = "æ˜æ˜¾æ”¾é‡"
            elif volume_change_pct > 3:
                volume_trend = "æ”¾é‡"
            elif volume_change_pct < -10:
                volume_trend = "æ˜æ˜¾ç¼©é‡"
            elif volume_change_pct < -3:
                volume_trend = "ç¼©é‡"
            else:
                volume_trend = "å¹³ç¨³"
        else:
            volume_change_pct = 0
            volume_trend = "å¹³ç¨³"

        # è®¡ç®—ä¸å‡å€¼çš„åç¦»ç¨‹åº¦
        if std_vma5 > 0:
            z_score = (current_vma5 - mean_vma5) / std_vma5
        else:
            z_score = 0

        return {
            "å½“å‰5æ—¥æˆäº¤é‡å‡çº¿": round(current_vma5, 2),
            "æˆäº¤é‡ç™¾åˆ†ä½": round(percentile, 2),
            "æˆäº¤é‡çŠ¶æ€": volume_status,
            "æˆäº¤é‡ç­‰çº§": volume_level,  # 1-5ï¼Œæ•°å­—è¶Šå¤§æˆäº¤é‡è¶Šé«˜
            "æˆäº¤é‡è¶‹åŠ¿": volume_trend,
            "æˆäº¤é‡å˜åŒ–ç‡": round(volume_change_pct, 2),
            "å›çœ‹æœŸæœ€å¤§å€¼": round(max_vma5, 2),
            "å›çœ‹æœŸæœ€å°å€¼": round(min_vma5, 2),
            "å›çœ‹æœŸå‡å€¼": round(mean_vma5, 2),
            "Zåˆ†æ•°": round(z_score, 2),  # æ ‡å‡†åˆ†æ•°ï¼Œåæ˜ åç¦»å‡å€¼çš„ç¨‹åº¦
            "è·ç¦»æœ€é«˜ç‚¹": round(((max_vma5 - current_vma5) / max_vma5) * 100, 2),
            "è·ç¦»æœ€ä½ç‚¹": round(((current_vma5 - min_vma5) / min_vma5) * 100, 2),
        }


    def get_investment_advice(self, rating, signal_count):
        """æ ¹æ®ç»¼åˆè¯„çº§ç»™å‡ºæŠ•èµ„å»ºè®®"""
        if rating == "å¼ºçƒˆè¶…è·Œ":
            return "ğŸ”¥ å¤šé‡æŠ€æœ¯æŒ‡æ ‡æ˜¾ç¤ºå¼ºçƒˆè¶…è·Œï¼Œå¯è€ƒè™‘åˆ†æ‰¹å»ºä»“ï¼Œä½†éœ€æ³¨æ„é£é™©æ§åˆ¶"
        elif rating == "å¯èƒ½è¶…è·Œ":
            return "âš ï¸ æŠ€æœ¯æŒ‡æ ‡æ˜¾ç¤ºå¯èƒ½è¶…è·Œï¼Œå¯å°é‡è¯•æ¢å»ºä»“ï¼Œå¯†åˆ‡å…³æ³¨åç»­èµ°åŠ¿"
        elif rating == "è§‚æœ›":
            return "ğŸ‘€ éƒ¨åˆ†æŠ€æœ¯æŒ‡æ ‡æ˜¾ç¤ºè°ƒæ•´ï¼Œå»ºè®®è§‚æœ›ç­‰å¾…æ›´å¥½æœºä¼š"
        else:
            return "âœ… æŠ€æœ¯æŒ‡æ ‡ç›¸å¯¹æ­£å¸¸ï¼Œå¯æŒ‰æ—¢å®šç­–ç•¥æ“ä½œ"

    def analyze_instruments_from_macd_file(self, instrument_type, date_str=None):
        """
        ä»MACDä¿¡å·æ–‡ä»¶è¯»å–æ•°æ®å¹¶æ‰§è¡Œç»¼åˆæŠ€æœ¯åˆ†æ

        Args:
            instrument_type: äº§å“ç±»å‹ ('industry_sector', 'stock', 'etf', 'concept_sector', 'index')
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä»Šå¤©

        Returns:
            dict: åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
        """
        from datetime import datetime

        if date_str is None:
            date_str = datetime.now().strftime("%Y-%m-%d")

        # ä½¿ç”¨FilePathGeneratorç”Ÿæˆæ–‡ä»¶è·¯å¾„
        filepath = FilePathGenerator.generate_macd_signal_path(
            instrument_type=instrument_type,
            period="30m",
            date=date_str
        )

        print(f"è¯»å–MACDä¿¡å·æ–‡ä»¶: {filepath}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(filepath):
            return {"error": f"MACDä¿¡å·æ–‡ä»¶ä¸å­˜åœ¨: {filepath}"}

        try:
            # è¯»å–CSVæ–‡ä»¶ï¼ŒæŒ‡å®šcodeåˆ—ä¸ºå­—ç¬¦ä¸²ä»¥ä¿ç•™å‰å¯¼é›¶
            macd_data = pd.read_csv(filepath, dtype={'code': str})
            if macd_data.empty:
                return {"error": f"MACDä¿¡å·æ–‡ä»¶ä¸ºç©º: {filepath}"}

            print(f"æˆåŠŸè¯»å– {len(macd_data)} æ¡MACDä¿¡å·æ•°æ®")

            # è·å–æ‰€æœ‰ç‹¬ç‰¹çš„è‚¡ç¥¨ä»£ç ä½œä¸ºåˆ—è¡¨å˜é‡
            if 'code' in macd_data.columns:
                instrument_codes = macd_data['code'].unique().tolist()
                print(f"å‘ç° {len(instrument_codes)} ä¸ªç‹¬ç‰¹çš„é‡‘èäº§å“ä»£ç ")
            else:
                return {"error": "MACDä¿¡å·æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°'code'åˆ—"}

            # ä¸ºæ¯ä¸ªä»£ç æ‰§è¡Œç»¼åˆæŠ€æœ¯åˆ†æ
            all_analysis_results = []
            successful_analyses = 0
            failed_analyses = 0

            for code in instrument_codes:
                try:
                    print(f"æ­£åœ¨åˆ†æ: {code}")

                    # æ‰§è¡Œç»¼åˆæŠ€æœ¯åˆ†æ
                    analysis_result = self.analyze_comprehensive_technical(
                        code=code,
                        data_source=instrument_type.replace('_sector', '')  # è½¬æ¢ä¸ºæ•°æ®æºæ ¼å¼
                    )

                    if "error" not in analysis_result:
                        analysis_result["åˆ†ææ¥æº"] = "MACDä¿¡å·æ–‡ä»¶"
                        analysis_result["MACDä¿¡å·æ—¥æœŸ"] = date_str
                        analysis_result["äº§å“ç±»å‹"] = instrument_type
                        all_analysis_results.append(analysis_result)
                        successful_analyses += 1
                        print(f"âœ“ {code} åˆ†æå®Œæˆ")
                    else:
                        print(f"âœ— {code} åˆ†æå¤±è´¥: {analysis_result['error']}")
                        failed_analyses += 1

                except Exception as e:
                    print(f"âœ— {code} åˆ†æå¼‚å¸¸: {str(e)}")
                    failed_analyses += 1
                    continue

            # ç”Ÿæˆç»“æœæ‘˜è¦
            summary = {
                "åˆ†ææ—¥æœŸ": date_str,
                "äº§å“ç±»å‹": instrument_type,
                "æ€»äº§å“æ•°é‡": len(instrument_codes),
                "æˆåŠŸåˆ†ææ•°é‡": successful_analyses,
                "å¤±è´¥åˆ†ææ•°é‡": failed_analyses,
                "åˆ†ææˆåŠŸç‡": f"{(successful_analyses / len(instrument_codes) * 100):.1f}%" if instrument_codes else "0%"
            }

            # å°†å®Œæ•´ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶
            result_data = {
                "æ‘˜è¦": summary,
                "åˆ†æç»“æœ": all_analysis_results
            }

            # ç”ŸæˆJSONæ–‡ä»¶è·¯å¾„
            json_filepath = f"data/{instrument_type}_comprehensive_analysis_{date_str}.json"

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            FilePathGenerator.ensure_directory_exists(json_filepath)

            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2, default=str)

            print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°JSONæ–‡ä»¶: {json_filepath}")
            print(f"åˆ†æå®Œæˆ: æˆåŠŸ {successful_analyses} ä¸ªï¼Œå¤±è´¥ {failed_analyses} ä¸ª")

            return result_data

        except Exception as e:
            return {"error": f"å¤„ç†MACDä¿¡å·æ–‡ä»¶å¤±è´¥: {str(e)}"}


# å…¼å®¹æ€§ç±»ï¼Œä¿æŒåŸæœ‰æ¥å£
class IndustryDataCollector(UnifiedDataCollector):
    """è¡Œä¸šæ•°æ®æ”¶é›†å™¨ï¼ˆå…¼å®¹æ€§ç±»ï¼‰"""
    
    def __init__(self):
        super().__init__()
        # ä¿æŒåŸæœ‰æ–¹æ³•å…¼å®¹æ€§
        self.db = self.industry_sector.db
    
    def get_all_boards(self):
        """è·å–æ‰€æœ‰æ¿å—åç§°å’Œä»£ç ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        boards = self.industry_sector.get_all_instruments()
        # è½¬æ¢ä¸ºåŸæœ‰æ ¼å¼
        return [{'æ¿å—åç§°': board['name'], 'æ¿å—ä»£ç ': board['code']} for board in boards]
    
    def get_historical_min_data(self, board_name, period="5"):
        """è·å–æŒ‡å®šæ¿å—çš„å†å²åˆ†æ—¶æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.industry_sector.get_historical_min_data({'name': board_name}, period)

    def get_historical_5min_data(self, board_name, period="5"):
        """è·å–æŒ‡å®šæ¿å—çš„5åˆ†é’Ÿå†å²æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.get_historical_min_data(board_name, period)

    def save_historical_min_data(self, board_info, data, period="5"):
        """ä¿å­˜å†å²åˆ†æ—¶æ•°æ®åˆ°æ•°æ®åº“ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.industry_sector.save_historical_min_data(board_info, data, period)

    def save_historical_5min_data(self, board_info, data, period="5"):
        """ä¿å­˜5åˆ†é’Ÿå†å²æ•°æ®åˆ°æ•°æ®åº“ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.save_historical_min_data(board_info, data, period)
    
    def get_realtime_1min_data(self):
        """è·å–1åˆ†é’Ÿå®æ—¶æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.industry_sector.get_realtime_1min_data()
    
    def collect_all_historical_min_data(self, period="5", delay_seconds=None):
        """æ”¶é›†æ‰€æœ‰æ¿å—å†å²åˆ†æ—¶æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.industry_sector.collect_all_historical_min_data(period, delay_seconds)

    def collect_all_historical_5min_data(self, delay_seconds=None):
        """æ”¶é›†æ‰€æœ‰æ¿å—5åˆ†é’Ÿå†å²æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.collect_all_historical_min_data("5", delay_seconds)

    def collect_all_daily_data(self, delay_seconds=None):
        """æ”¶é›†æ‰€æœ‰æ¿å—æ—¥Kæ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.industry_sector.collect_all_daily_data(delay_seconds)
    
    # ä¿æŒåŸæœ‰æ–¹æ³•åç§°ä»¥å…¼å®¹æ—§ä»£ç 
    def get_historical_data(self, board_name, period="5"):
        """è·å–æŒ‡å®šæ¿å—çš„å†å²æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.get_historical_min_data(board_name, period)

    def save_historical_data(self, board_info, data, period="5"):
        """ä¿å­˜å†å²æ•°æ®åˆ°æ•°æ®åº“ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.save_historical_min_data(board_info, data, period)

    def get_realtime_data(self):
        """è·å–å®æ—¶æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.get_realtime_1min_data()

    def collect_all_historical_data(self, period="5", delay_seconds=None):
        """æ”¶é›†æ‰€æœ‰æ¿å—å†å²æ•°æ®ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.collect_all_historical_min_data(period, delay_seconds)


class IndustryAnalyzer(UnifiedAnalyzer):
    """è¡Œä¸šåˆ†æå™¨ï¼ˆå…¼å®¹æ€§ç±»ï¼‰"""
    
    def __init__(self, data_collector=None):
        super().__init__()
        if data_collector is None:
            self.data_collector = IndustryDataCollector()
        else:
            self.data_collector = data_collector
    
    def resample_data(self, data, period):
        """é‡é‡‡æ ·æ•°æ®åˆ°æŒ‡å®šå‘¨æœŸï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.industry_sector.resample_data(data, period)
    
    def calculate_macd(self, close_prices, fast=12, slow=26, signal=9):
        """è®¡ç®—MACDæŒ‡æ ‡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.industry_sector.calculate_macd(close_prices, fast, slow, signal)
    
    def detect_macd_signals(self, macd_line, signal_line):
        """æ£€æµ‹MACDä¿¡å·ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.industry_sector.detect_macd_signals(macd_line, signal_line)
    
    def analyze_board_macd(self, board_info):
        """åˆ†æå•ä¸ªæ¿å—çš„MACDï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.industry_sector.analyze_macd(board_info)
    
    def analyze_all_boards(self):
        """åˆ†ææ‰€æœ‰æ¿å—çš„MACDï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.analyze_all_instruments('industry_sector')
    
    def is_price_at_monthly_high_drawdown_5pct(self, board_name, current_price=None):
        """è®¡ç®—æœˆåº¦å›æ’¤ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        # åˆ›å»ºä¸´æ—¶board_info
        board_info = {'name': board_name, 'code': board_name}
        
        data = self.industry_sector.combine_historical_and_realtime(board_info)
        
        if data is None or data.empty:
            print(f"{board_name}: æ— æ³•è·å–æ•°æ®")
            return None
        
        from datetime import timedelta
        current_time = datetime.now()
        one_month_ago = current_time - timedelta(days=30)
        
        data['æ—¥æœŸæ—¶é—´'] = pd.to_datetime(data['æ—¥æœŸæ—¶é—´'])
        monthly_data = data[data['æ—¥æœŸæ—¶é—´'] >= one_month_ago].copy()
        
        if monthly_data.empty:
            print(f"{board_name}: ä¸€ä¸ªæœˆå†…æ²¡æœ‰æ•°æ®")
            return None
        
        if current_price is None:
            current_price = monthly_data['æ”¶ç›˜'].iloc[-1]
        
        monthly_high = monthly_data['æœ€é«˜'].max()
        high_date_idx = monthly_data['æœ€é«˜'].idxmax()
        high_date = monthly_data.loc[high_date_idx, 'æ—¥æœŸæ—¶é—´']
        
        drawdown_5pct_price = monthly_high * 0.95
        actual_drawdown_pct = ((monthly_high - current_price) / monthly_high) * 100
        days_from_high = (current_time - high_date).days
        
        is_at_drawdown_5pct = abs(actual_drawdown_pct - 5.0) <= 1.0 and actual_drawdown_pct >= 4.0
        
        result = {
            'is_at_drawdown_5pct': is_at_drawdown_5pct,
            'current_price': current_price,
            'monthly_high': monthly_high,
            'drawdown_5pct_price': drawdown_5pct_price,
            'actual_drawdown_pct': actual_drawdown_pct,
            'days_from_high': days_from_high,
            'high_date': high_date
        }
        
        return result